{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook I will train and evaluate two different Trigram models.\n",
    "More precisely I will use two different approaches for creating a Trigram model.\n",
    "Approach 1 is to build a matrix that holds the true probability distribution of which character \n",
    "comes next given that the model has been given two particular preceeding characters.\n",
    "We then sample from this distribution to produce the next character in the output 'name'.\n",
    "\n",
    "The second approach is to use a Neural Net. We take in the two preceeding characters to generate the next one \n",
    "and so on. The goal is to achieve the same performance or loss using the Neural Net approach as with approach 1.\n",
    "As you will see approach 1 is the perfect approach given the loss function I will use. \n",
    "It is impossible to achieve better loss with a Neural Net approach to the Trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in all the names from the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names_dataset.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A peak at some names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting up the Dataset into a Train and Test split. 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Names: 25626\n",
      "Number of Testing Names: 6407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_names, test_names = train_test_split(words, test_size=0.2, random_state=42)  # 20% for testing\n",
    "\n",
    "print(\"Number of Training Names:\", len(train_names))\n",
    "print(\"Number of Testing Names:\", len(test_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the matrix that will hold the counts for future characters,\n",
    "please observe that we need 256 rows because for 27 characters the possible\n",
    "permutations of 2 characters following each other is 27^2 = 27 * 27 = 729.\n",
    "But I will actually use 729 - 27 = 720 rows because I will ignore all entries\n",
    "that has the * character as the last because if that has happened the model has already stopped producing output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([702, 27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every row in this matrix will eventually be filled with the prob-distribution of the next characters given two particular preceeding characters\n",
    "N = torch.zeros((702, 27), dtype=torch.int32)\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two mappings between integers and characters so I can represent every characters with an integer in computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '*',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars.insert(0, '*')\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "stoi\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two mappings between pairs of characters and Integers. \n",
    "Ignoring all entries that end with *, because if that happens the model has already stopped producing output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*a': 0, '*b': 1, '*c': 2, '*d': 3, '*e': 4, '*f': 5, '*g': 6, '*h': 7, '*i': 8, '*j': 9, '*k': 10, '*l': 11, '*m': 12, '*n': 13, '*o': 14, '*p': 15, '*q': 16, '*r': 17, '*s': 18, '*t': 19, '*u': 20, '*v': 21, '*w': 22, '*x': 23, '*y': 24, '*z': 25, 'aa': 26, 'ab': 27, 'ac': 28, 'ad': 29, 'ae': 30, 'af': 31, 'ag': 32, 'ah': 33, 'ai': 34, 'aj': 35, 'ak': 36, 'al': 37, 'am': 38, 'an': 39, 'ao': 40, 'ap': 41, 'aq': 42, 'ar': 43, 'as': 44, 'at': 45, 'au': 46, 'av': 47, 'aw': 48, 'ax': 49, 'ay': 50, 'az': 51, 'ba': 52, 'bb': 53, 'bc': 54, 'bd': 55, 'be': 56, 'bf': 57, 'bg': 58, 'bh': 59, 'bi': 60, 'bj': 61, 'bk': 62, 'bl': 63, 'bm': 64, 'bn': 65, 'bo': 66, 'bp': 67, 'bq': 68, 'br': 69, 'bs': 70, 'bt': 71, 'bu': 72, 'bv': 73, 'bw': 74, 'bx': 75, 'by': 76, 'bz': 77, 'ca': 78, 'cb': 79, 'cc': 80, 'cd': 81, 'ce': 82, 'cf': 83, 'cg': 84, 'ch': 85, 'ci': 86, 'cj': 87, 'ck': 88, 'cl': 89, 'cm': 90, 'cn': 91, 'co': 92, 'cp': 93, 'cq': 94, 'cr': 95, 'cs': 96, 'ct': 97, 'cu': 98, 'cv': 99, 'cw': 100, 'cx': 101, 'cy': 102, 'cz': 103, 'da': 104, 'db': 105, 'dc': 106, 'dd': 107, 'de': 108, 'df': 109, 'dg': 110, 'dh': 111, 'di': 112, 'dj': 113, 'dk': 114, 'dl': 115, 'dm': 116, 'dn': 117, 'do': 118, 'dp': 119, 'dq': 120, 'dr': 121, 'ds': 122, 'dt': 123, 'du': 124, 'dv': 125, 'dw': 126, 'dx': 127, 'dy': 128, 'dz': 129, 'ea': 130, 'eb': 131, 'ec': 132, 'ed': 133, 'ee': 134, 'ef': 135, 'eg': 136, 'eh': 137, 'ei': 138, 'ej': 139, 'ek': 140, 'el': 141, 'em': 142, 'en': 143, 'eo': 144, 'ep': 145, 'eq': 146, 'er': 147, 'es': 148, 'et': 149, 'eu': 150, 'ev': 151, 'ew': 152, 'ex': 153, 'ey': 154, 'ez': 155, 'fa': 156, 'fb': 157, 'fc': 158, 'fd': 159, 'fe': 160, 'ff': 161, 'fg': 162, 'fh': 163, 'fi': 164, 'fj': 165, 'fk': 166, 'fl': 167, 'fm': 168, 'fn': 169, 'fo': 170, 'fp': 171, 'fq': 172, 'fr': 173, 'fs': 174, 'ft': 175, 'fu': 176, 'fv': 177, 'fw': 178, 'fx': 179, 'fy': 180, 'fz': 181, 'ga': 182, 'gb': 183, 'gc': 184, 'gd': 185, 'ge': 186, 'gf': 187, 'gg': 188, 'gh': 189, 'gi': 190, 'gj': 191, 'gk': 192, 'gl': 193, 'gm': 194, 'gn': 195, 'go': 196, 'gp': 197, 'gq': 198, 'gr': 199, 'gs': 200, 'gt': 201, 'gu': 202, 'gv': 203, 'gw': 204, 'gx': 205, 'gy': 206, 'gz': 207, 'ha': 208, 'hb': 209, 'hc': 210, 'hd': 211, 'he': 212, 'hf': 213, 'hg': 214, 'hh': 215, 'hi': 216, 'hj': 217, 'hk': 218, 'hl': 219, 'hm': 220, 'hn': 221, 'ho': 222, 'hp': 223, 'hq': 224, 'hr': 225, 'hs': 226, 'ht': 227, 'hu': 228, 'hv': 229, 'hw': 230, 'hx': 231, 'hy': 232, 'hz': 233, 'ia': 234, 'ib': 235, 'ic': 236, 'id': 237, 'ie': 238, 'if': 239, 'ig': 240, 'ih': 241, 'ii': 242, 'ij': 243, 'ik': 244, 'il': 245, 'im': 246, 'in': 247, 'io': 248, 'ip': 249, 'iq': 250, 'ir': 251, 'is': 252, 'it': 253, 'iu': 254, 'iv': 255, 'iw': 256, 'ix': 257, 'iy': 258, 'iz': 259, 'ja': 260, 'jb': 261, 'jc': 262, 'jd': 263, 'je': 264, 'jf': 265, 'jg': 266, 'jh': 267, 'ji': 268, 'jj': 269, 'jk': 270, 'jl': 271, 'jm': 272, 'jn': 273, 'jo': 274, 'jp': 275, 'jq': 276, 'jr': 277, 'js': 278, 'jt': 279, 'ju': 280, 'jv': 281, 'jw': 282, 'jx': 283, 'jy': 284, 'jz': 285, 'ka': 286, 'kb': 287, 'kc': 288, 'kd': 289, 'ke': 290, 'kf': 291, 'kg': 292, 'kh': 293, 'ki': 294, 'kj': 295, 'kk': 296, 'kl': 297, 'km': 298, 'kn': 299, 'ko': 300, 'kp': 301, 'kq': 302, 'kr': 303, 'ks': 304, 'kt': 305, 'ku': 306, 'kv': 307, 'kw': 308, 'kx': 309, 'ky': 310, 'kz': 311, 'la': 312, 'lb': 313, 'lc': 314, 'ld': 315, 'le': 316, 'lf': 317, 'lg': 318, 'lh': 319, 'li': 320, 'lj': 321, 'lk': 322, 'll': 323, 'lm': 324, 'ln': 325, 'lo': 326, 'lp': 327, 'lq': 328, 'lr': 329, 'ls': 330, 'lt': 331, 'lu': 332, 'lv': 333, 'lw': 334, 'lx': 335, 'ly': 336, 'lz': 337, 'ma': 338, 'mb': 339, 'mc': 340, 'md': 341, 'me': 342, 'mf': 343, 'mg': 344, 'mh': 345, 'mi': 346, 'mj': 347, 'mk': 348, 'ml': 349, 'mm': 350, 'mn': 351, 'mo': 352, 'mp': 353, 'mq': 354, 'mr': 355, 'ms': 356, 'mt': 357, 'mu': 358, 'mv': 359, 'mw': 360, 'mx': 361, 'my': 362, 'mz': 363, 'na': 364, 'nb': 365, 'nc': 366, 'nd': 367, 'ne': 368, 'nf': 369, 'ng': 370, 'nh': 371, 'ni': 372, 'nj': 373, 'nk': 374, 'nl': 375, 'nm': 376, 'nn': 377, 'no': 378, 'np': 379, 'nq': 380, 'nr': 381, 'ns': 382, 'nt': 383, 'nu': 384, 'nv': 385, 'nw': 386, 'nx': 387, 'ny': 388, 'nz': 389, 'oa': 390, 'ob': 391, 'oc': 392, 'od': 393, 'oe': 394, 'of': 395, 'og': 396, 'oh': 397, 'oi': 398, 'oj': 399, 'ok': 400, 'ol': 401, 'om': 402, 'on': 403, 'oo': 404, 'op': 405, 'oq': 406, 'or': 407, 'os': 408, 'ot': 409, 'ou': 410, 'ov': 411, 'ow': 412, 'ox': 413, 'oy': 414, 'oz': 415, 'pa': 416, 'pb': 417, 'pc': 418, 'pd': 419, 'pe': 420, 'pf': 421, 'pg': 422, 'ph': 423, 'pi': 424, 'pj': 425, 'pk': 426, 'pl': 427, 'pm': 428, 'pn': 429, 'po': 430, 'pp': 431, 'pq': 432, 'pr': 433, 'ps': 434, 'pt': 435, 'pu': 436, 'pv': 437, 'pw': 438, 'px': 439, 'py': 440, 'pz': 441, 'qa': 442, 'qb': 443, 'qc': 444, 'qd': 445, 'qe': 446, 'qf': 447, 'qg': 448, 'qh': 449, 'qi': 450, 'qj': 451, 'qk': 452, 'ql': 453, 'qm': 454, 'qn': 455, 'qo': 456, 'qp': 457, 'qq': 458, 'qr': 459, 'qs': 460, 'qt': 461, 'qu': 462, 'qv': 463, 'qw': 464, 'qx': 465, 'qy': 466, 'qz': 467, 'ra': 468, 'rb': 469, 'rc': 470, 'rd': 471, 're': 472, 'rf': 473, 'rg': 474, 'rh': 475, 'ri': 476, 'rj': 477, 'rk': 478, 'rl': 479, 'rm': 480, 'rn': 481, 'ro': 482, 'rp': 483, 'rq': 484, 'rr': 485, 'rs': 486, 'rt': 487, 'ru': 488, 'rv': 489, 'rw': 490, 'rx': 491, 'ry': 492, 'rz': 493, 'sa': 494, 'sb': 495, 'sc': 496, 'sd': 497, 'se': 498, 'sf': 499, 'sg': 500, 'sh': 501, 'si': 502, 'sj': 503, 'sk': 504, 'sl': 505, 'sm': 506, 'sn': 507, 'so': 508, 'sp': 509, 'sq': 510, 'sr': 511, 'ss': 512, 'st': 513, 'su': 514, 'sv': 515, 'sw': 516, 'sx': 517, 'sy': 518, 'sz': 519, 'ta': 520, 'tb': 521, 'tc': 522, 'td': 523, 'te': 524, 'tf': 525, 'tg': 526, 'th': 527, 'ti': 528, 'tj': 529, 'tk': 530, 'tl': 531, 'tm': 532, 'tn': 533, 'to': 534, 'tp': 535, 'tq': 536, 'tr': 537, 'ts': 538, 'tt': 539, 'tu': 540, 'tv': 541, 'tw': 542, 'tx': 543, 'ty': 544, 'tz': 545, 'ua': 546, 'ub': 547, 'uc': 548, 'ud': 549, 'ue': 550, 'uf': 551, 'ug': 552, 'uh': 553, 'ui': 554, 'uj': 555, 'uk': 556, 'ul': 557, 'um': 558, 'un': 559, 'uo': 560, 'up': 561, 'uq': 562, 'ur': 563, 'us': 564, 'ut': 565, 'uu': 566, 'uv': 567, 'uw': 568, 'ux': 569, 'uy': 570, 'uz': 571, 'va': 572, 'vb': 573, 'vc': 574, 'vd': 575, 've': 576, 'vf': 577, 'vg': 578, 'vh': 579, 'vi': 580, 'vj': 581, 'vk': 582, 'vl': 583, 'vm': 584, 'vn': 585, 'vo': 586, 'vp': 587, 'vq': 588, 'vr': 589, 'vs': 590, 'vt': 591, 'vu': 592, 'vv': 593, 'vw': 594, 'vx': 595, 'vy': 596, 'vz': 597, 'wa': 598, 'wb': 599, 'wc': 600, 'wd': 601, 'we': 602, 'wf': 603, 'wg': 604, 'wh': 605, 'wi': 606, 'wj': 607, 'wk': 608, 'wl': 609, 'wm': 610, 'wn': 611, 'wo': 612, 'wp': 613, 'wq': 614, 'wr': 615, 'ws': 616, 'wt': 617, 'wu': 618, 'wv': 619, 'ww': 620, 'wx': 621, 'wy': 622, 'wz': 623, 'xa': 624, 'xb': 625, 'xc': 626, 'xd': 627, 'xe': 628, 'xf': 629, 'xg': 630, 'xh': 631, 'xi': 632, 'xj': 633, 'xk': 634, 'xl': 635, 'xm': 636, 'xn': 637, 'xo': 638, 'xp': 639, 'xq': 640, 'xr': 641, 'xs': 642, 'xt': 643, 'xu': 644, 'xv': 645, 'xw': 646, 'xx': 647, 'xy': 648, 'xz': 649, 'ya': 650, 'yb': 651, 'yc': 652, 'yd': 653, 'ye': 654, 'yf': 655, 'yg': 656, 'yh': 657, 'yi': 658, 'yj': 659, 'yk': 660, 'yl': 661, 'ym': 662, 'yn': 663, 'yo': 664, 'yp': 665, 'yq': 666, 'yr': 667, 'ys': 668, 'yt': 669, 'yu': 670, 'yv': 671, 'yw': 672, 'yx': 673, 'yy': 674, 'yz': 675, 'za': 676, 'zb': 677, 'zc': 678, 'zd': 679, 'ze': 680, 'zf': 681, 'zg': 682, 'zh': 683, 'zi': 684, 'zj': 685, 'zk': 686, 'zl': 687, 'zm': 688, 'zn': 689, 'zo': 690, 'zp': 691, 'zq': 692, 'zr': 693, 'zs': 694, 'zt': 695, 'zu': 696, 'zv': 697, 'zw': 698, 'zx': 699, 'zy': 700, 'zz': 701}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars.insert(0, '*')\n",
    "\n",
    "twochars = []\n",
    "for char1 in chars:\n",
    "    for char2 in chars:\n",
    "        if char2 != '*':\n",
    "            twochars.append(f'{char1}{char2}')\n",
    "\n",
    "doublettoi = {s:i for i, s in enumerate(twochars)}\n",
    "itodoublet = {i:s for s,i in doublettoi.items()}\n",
    "print(doublettoi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to populate our matrix with counts of future characters!\n",
    "I will use * to signal beginning and end of string. This will also be treated as a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in train_names:\n",
    "    chs = ['*'] + list(w) + ['*']\n",
    "    for i in range(len(chs) - 2):\n",
    "        doublet = chs[i] + chs[i + 1]\n",
    "        ch2 = chs[i + 2]\n",
    "        ix1 = doublettoi[doublet]\n",
    "        ix2 = stoi[ch2]\n",
    "        # print(f\"{doublet}, {ch2}\")\n",
    "        N[ix1, ix2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a tiny bit of smoothing to N so we dont get any zero-probabilities:\n",
    "N += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will visualize our matrix that holds the counts. Darker blue indicates more instances and white indicates no instances.\n",
    "Beware that the matrix plot is very long since we have so many permutations of two characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16, 420))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(702):\n",
    "    for j in range(27):\n",
    "        chstr = itodoublet[i] + ' ' + itos[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert this Count Matrix to a Probability Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([702, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = (N).float()\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to convert the Count-Matrix to a probability-Matrix. Now every row will contain a probability distribution based on the counts of that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will visualize the probabilites for the next character given the previous two characters. Please beware that this matrix is very long so after you've had a look at the matrix maybe clear the ouput so you don't have to scroll a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16, 420))\n",
    "plt.imshow(P, cmap='Blues')\n",
    "for i in range(702):\n",
    "    for j in range(27):\n",
    "        chstr = itodoublet[i] + ' ' + itos[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, \"{:.3f}\".format(P[i, j].item()), ha='center', va='top', color='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I will generate names I will find the probability distribution of the first character in all the names. This is because for our Trigram model to be able to start generating outputs we need two input characters. We only have '*' in the beginning so we need a first character also. We can generate a random first character but this will produce far more unnatural names rather than if we sample from the true distribution of the first characters in our dataset. Therefore we perform the following step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'i': 479, 'm': 2005, 'k': 2360, 'e': 1215, 'c': 1245, 'l': 1252, 'a': 3479, 't': 1063, 'z': 751, 'n': 923, 'r': 1344, 's': 1647, 'b': 1050, 'p': 429, 'f': 334, 'j': 1910, 'v': 304, 'g': 544, 'd': 1326, 'o': 309, 'h': 720, 'y': 428, 'w': 256, 'q': 79, 'x': 110, 'u': 64}\n",
      "Probabilities: {'i': 0.01869195348474206, 'm': 0.07824084913759463, 'k': 0.09209396706469991, 'e': 0.04741278389136034, 'c': 0.04858346991336923, 'l': 0.04885662998517131, 'a': 0.13576055568563178, 't': 0.04148130804651526, 'z': 0.029306173417622726, 'n': 0.036018106610473735, 'r': 0.052446733785998595, 's': 0.06427066260828845, 'b': 0.0409740107703114, 'p': 0.01674081011472723, 'f': 0.013033637711699056, 'j': 0.07453367673456646, 'v': 0.01186295168969016, 'g': 0.021228439865761336, 'd': 0.051744322172793254, 'o': 0.012058066026691641, 'h': 0.028096464528213533, 'y': 0.016701787247326932, 'w': 0.009989854054475923, 'q': 0.0030828065246234293, 'x': 0.0042925154140326235, 'u': 0.002497463513618981}\n",
      "Probability Tensor: tensor([0.1358, 0.0410, 0.0486, 0.0517, 0.0474, 0.0130, 0.0212, 0.0281, 0.0187,\n",
      "        0.0745, 0.0921, 0.0489, 0.0782, 0.0360, 0.0121, 0.0167, 0.0031, 0.0524,\n",
      "        0.0643, 0.0415, 0.0025, 0.0119, 0.0100, 0.0043, 0.0167, 0.0293])\n"
     ]
    }
   ],
   "source": [
    "names = train_names\n",
    "\n",
    "# Initializing a dictionary\n",
    "first_char_counts = {}\n",
    "\n",
    "# Counting each first character\n",
    "for name in names:\n",
    "    if name:\n",
    "        first_char = name[0].lower()\n",
    "        if first_char in first_char_counts:\n",
    "            first_char_counts[first_char] += 1\n",
    "        else:\n",
    "            first_char_counts[first_char] = 1\n",
    "\n",
    "# Converting counts to probabilities\n",
    "total_names = len(names)\n",
    "first_char_probabilities = {char: count / total_names for char, count in first_char_counts.items()}\n",
    "\n",
    "# Printing the results\n",
    "print(\"Counts:\", first_char_counts)\n",
    "print(\"Probabilities:\", first_char_probabilities)\n",
    "\n",
    "# Plucking out the probabilities for each character\n",
    "sorted_probs = sorted(first_char_probabilities.items())  \n",
    "prob_values = [prob for char, prob in sorted_probs]  \n",
    "\n",
    "# Converting to a PyTorch tensor\n",
    "first_character_probs = torch.tensor(prob_values, dtype=torch.float32)\n",
    "\n",
    "# Printing the tensor\n",
    "print(\"Probability Tensor:\", first_character_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sample from P to get the next character until we get *, in that case we stop generating and we break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  munth\n",
      "Name:  resaillindynklyn\n",
      "Name:  ilendrnnia\n",
      "Name:  dana\n",
      "Name:  ber\n",
      "Name:  ganna\n",
      "Name:  julin\n",
      "Name:  lormogurni\n",
      "Name:  emmere\n",
      "Name:  ivwrean\n",
      "Name:  gadannogpjxjeryah\n",
      "Name:  ley\n",
      "Name:  joelleenah\n",
      "Name:  louray\n",
      "Name:  ricken\n",
      "Name:  ela\n",
      "Name:  bentrukwu\n",
      "Name:  fis\n",
      "Name:  riononna\n",
      "Name:  urosabiq\n",
      "Name:  fioroie\n",
      "Name:  que\n",
      "Name:  quindannyerevueld\n",
      "Name:  raharic\n",
      "Name:  caray\n",
      "Name:  faivi\n",
      "Name:  qdsynnjaziesminluwen\n",
      "Name:  jen\n",
      "Name:  reniekileelyn\n",
      "Name:  lano\n",
      "Name:  rian\n",
      "Name:  re\n",
      "Name:  atcjldmzbres\n",
      "Name:  graelie\n",
      "Name:  jercmla\n",
      "Name:  berne\n",
      "Name:  isczrvihuryn\n",
      "Name:  qzana\n",
      "Name:  fri\n"
     ]
    }
   ],
   "source": [
    "# We can generate 20 names for example:\n",
    "\n",
    "g = torch.Generator().manual_seed(423283494)\n",
    "\n",
    "for i in range(50):\n",
    "    out = []\n",
    "\n",
    "    # Sampling the first character:\n",
    "    first_char = torch.multinomial(first_character_probs, num_samples=1)\n",
    "    first_char = itos[first_char.item()]\n",
    "    if first_char == '*':\n",
    "        continue\n",
    "    first_doublet = f'*{first_char}'\n",
    "    first_doublet_index = torch.tensor(doublettoi[first_doublet])\n",
    "    out.append(first_char)\n",
    "    first_doublet = f'*{first_char}'\n",
    "\n",
    "    ix = doublettoi[first_doublet]\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # Pluck out the probability distribution for the next character\n",
    "        p = P[ix]\n",
    "\n",
    "        #print('ix: ', ix)\n",
    "        #print('current input doublet: ', itodoublet[ix])\n",
    "        # Sampling\n",
    "        next_char_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        #print('next char ix: ', next_char_ix)\n",
    "        if next_char_ix == 0:\n",
    "            break\n",
    "        #print('Adding to out: ', itos[next_char_ix])\n",
    "        out.append(itos[next_char_ix])\n",
    "        next_doublet = ''.join(out[-2:])\n",
    "        #print('next_doublet: ', next_doublet)\n",
    "\n",
    "        ix = doublettoi[next_doublet]\n",
    "\n",
    "    print('Name: ', ''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our Trigram model on our test-set and see what our Negative Log-likehihood is. We want this value (loss) to be as low as possible. A value of zero indicates that our model is 100% confident about every character in every word showing up when they actually do. In other words it would be able to perfectly predict the names in our dataset. This can't happen in practice but in theory that is kind of what we aim for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood is: -83431.7265625\n",
      "Negative Log Likelihood is: 83431.7265625\n",
      "Average Negative Log Likelihood is: 2.126082420349121\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "\n",
    "for w in test_names:\n",
    "    chs = ['*'] + list(w) + ['*']\n",
    "    for i in range(len(chs) - 2):\n",
    "        doublet = chs[i] + chs[i + 1]\n",
    "        ch2 = chs[i + 2]\n",
    "        ix1 = doublettoi[doublet]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        count += 1\n",
    "\n",
    "\n",
    "print(f\"Log Likelihood is: {log_likelihood}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"Negative Log Likelihood is: {nll}\")\n",
    "mean_nll = nll/count\n",
    "print(f\"Average Negative Log Likelihood is: {mean_nll}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the loss we achieve on the first approach using a probability matrix is roughly 2.126."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same thing but with a Neural Net instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  156871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  8, 247, 367,  ..., 472, 141, 323]),\n",
       " tensor([14,  4,  9,  ..., 12, 12,  0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Neural Net to solve the same task:\n",
    "\n",
    "# Creating the Train Dataset:\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in train_names:\n",
    "    chs = ['*'] + list(w) + ['*']\n",
    "    for i in range(len(chs) - 2):\n",
    "        doublet = chs[i] + chs[i + 1]\n",
    "        ch2 = chs[i + 2]\n",
    "        ix1 = doublettoi[doublet]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num_examples = xs.nelement()\n",
    "print('Number of examples: ', num_examples)\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Neurons will each receive 702 inputs and we will have 27 neurons in total producing an output with dim 1 x 27. We will interpret these outputs as a probability distribution over the next character given an input of a two-character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Neural Network:\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 702), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code that will process inputs with our neural net, calculate the loss, from the loss calculate the gradients and from the gradient update the weights in our Neural Net properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156871\n",
      "2.229524612426758\n",
      "2.2288460731506348\n",
      "2.22817325592041\n",
      "2.227505683898926\n",
      "2.22684383392334\n",
      "2.226186990737915\n",
      "2.2255356311798096\n",
      "2.2248895168304443\n",
      "2.2242486476898193\n",
      "2.2236125469207764\n",
      "2.2229814529418945\n",
      "2.222355365753174\n",
      "2.221734046936035\n",
      "2.2211177349090576\n",
      "2.220506429672241\n",
      "2.2198996543884277\n",
      "2.219297170639038\n",
      "2.2186996936798096\n",
      "2.218106746673584\n",
      "2.2175183296203613\n",
      "2.2169342041015625\n",
      "2.2163546085357666\n",
      "2.2157790660858154\n",
      "2.215207815170288\n",
      "2.2146413326263428\n",
      "2.214078664779663\n",
      "2.213519811630249\n",
      "2.212965488433838\n",
      "2.2124152183532715\n",
      "2.21186900138855\n",
      "2.2113265991210938\n",
      "2.210787773132324\n",
      "2.2102534770965576\n",
      "2.2097229957580566\n",
      "2.209196090698242\n",
      "2.2086727619171143\n",
      "2.208153009414673\n",
      "2.2076375484466553\n",
      "2.207125186920166\n",
      "2.2066168785095215\n",
      "2.2061116695404053\n",
      "2.2056102752685547\n",
      "2.2051119804382324\n",
      "2.204617500305176\n",
      "2.2041265964508057\n",
      "2.2036385536193848\n",
      "2.2031540870666504\n",
      "2.2026729583740234\n",
      "2.202195167541504\n",
      "2.2017204761505127\n",
      "2.20124888420105\n",
      "2.2007808685302734\n",
      "2.200315475463867\n",
      "2.1998534202575684\n",
      "2.1993942260742188\n",
      "2.1989383697509766\n",
      "2.198485851287842\n",
      "2.198035478591919\n",
      "2.1975886821746826\n",
      "2.1971445083618164\n",
      "2.1967034339904785\n",
      "2.19626522064209\n",
      "2.1958296298980713\n",
      "2.195396900177002\n",
      "2.194967031478882\n",
      "2.194540023803711\n",
      "2.19411563873291\n",
      "2.1936938762664795\n",
      "2.193274974822998\n",
      "2.1928582191467285\n",
      "2.1924448013305664\n",
      "2.192033529281616\n",
      "2.191624879837036\n",
      "2.191218852996826\n",
      "2.1908152103424072\n",
      "2.1904144287109375\n",
      "2.190016031265259\n",
      "2.189619541168213\n",
      "2.1892261505126953\n",
      "2.1888349056243896\n",
      "2.188446044921875\n",
      "2.1880595684051514\n",
      "2.1876754760742188\n",
      "2.187293529510498\n",
      "2.1869144439697266\n",
      "2.186537027359009\n",
      "2.186161994934082\n",
      "2.1857893466949463\n",
      "2.1854188442230225\n",
      "2.1850504875183105\n",
      "2.1846842765808105\n",
      "2.1843204498291016\n",
      "2.1839587688446045\n",
      "2.1835989952087402\n",
      "2.183241605758667\n",
      "2.1828858852386475\n",
      "2.182532548904419\n",
      "2.1821811199188232\n",
      "2.1818318367004395\n",
      "2.1814844608306885\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Performing Gradient Descent to improve the performance of the Network:\n",
    "num_examples = xs.nelement()\n",
    "print(num_examples)\n",
    "\n",
    "for k in range(100):\n",
    "\n",
    "    # Forward pass\n",
    "    # We encode our input with the one-hot encoding, creating a tensor that is filled with 701 zeros and 1 one to signal which of the possible\n",
    "    # 702 input combinations is currently being fed to the network.\n",
    "    xenc = F.one_hot(xs, num_classes=702).float()\n",
    "    \n",
    "    logits = xenc @ W.T # Logits prediction\n",
    "    counts = logits.exp() # Now we only have positive counts\n",
    "    probs = counts / counts.sum(1, keepdim=True) # Now we have the probabilities outputted from the Neural Net for next character\n",
    "    loss = -probs[torch.arange(num_examples), ys].log().mean() # The loss we use here is the Negative Log-likelihood Loss\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None # Set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    W.data += -60 * W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the Neural Network on the Test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  39242\n"
     ]
    }
   ],
   "source": [
    "# Creating the Test Dataset:\n",
    "\n",
    "xs_test, ys_test = [], []\n",
    "\n",
    "for w in test_names:\n",
    "    chs = ['*'] + list(w) + ['*']\n",
    "    for i in range(len(chs) - 2):\n",
    "        doublet = chs[i] + chs[i + 1]\n",
    "        ch2 = chs[i + 2]\n",
    "        ix1 = doublettoi[doublet]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs_test.append(ix1)\n",
    "        ys_test.append(ix2)\n",
    "\n",
    "xs_test = torch.tensor(xs_test)\n",
    "ys_test = torch.tensor(ys_test)\n",
    "\n",
    "num_examples = xs_test.nelement()\n",
    "print('Number of examples: ', num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39242\n",
      "2.2496397495269775\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Performing Gradient Descent to improve the performance of the Network:\n",
    "num_examples = xs_test.nelement()\n",
    "print(num_examples)\n",
    "\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "# We encode our input with the one-hot encoding, creating a tensor that is filled with 701 zeros and 1 one to signal which of the possible\n",
    "# 702 input combinations is currently being fed to the network.\n",
    "xenc = F.one_hot(xs_test, num_classes=702).float()\n",
    "\n",
    "logits = xenc @ W.T # Logits prediction\n",
    "counts = logits.exp() # Now we only have positive counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # Now we have the probabilities outputted from the Neural Net for next character\n",
    "loss = -probs[torch.arange(num_examples), ys_test].log().mean() # The loss we use here is the Negative Log-likelihood Loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the training code for a couple of times you can actually achieve lower loss on the test-set than approach 1. This is actually really interesting and I dont know quite why at this point. I did not think that this was possible actually. But my best loss for approach 1 was: 2.126 and loss for approach 2 was: 2.122."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simulate some names with approach 2 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reia\n",
      "aqvsxpummdyva\n",
      "niyantcruvu\n",
      "teryn\n",
      "kamisilugs\n",
      "asebz\n",
      "jadlatra\n",
      "marifpctjlporuccdvqnolia\n",
      "sanor\n",
      "vielikobi\n",
      "tama\n",
      "dan\n",
      "beo\n",
      "ty\n",
      "mayaabzoodqyn\n",
      "nyia\n",
      "milynne\n",
      "kath\n",
      "ah\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "\n",
    "\n",
    "    first_char = torch.multinomial(first_character_probs, num_samples=1)\n",
    "    first_char = itos[first_char.item()+1]\n",
    "    first_doublet = f'*{first_char}'\n",
    "    first_doublet_index = torch.tensor(doublettoi[first_doublet])\n",
    "\n",
    "    start_symbol = F.one_hot(first_doublet_index, num_classes=702).float() # Inputting start symbol to the Neural Network to make it produce output characters\n",
    "    current_doublet = start_symbol\n",
    "    # Initializing current name:\n",
    "    current_word = []\n",
    "    # Adding the first randomly generated character:\n",
    "    current_word.append(first_char)\n",
    "\n",
    "    # This variable will keep track of the index of the current input doublet:\n",
    "    next_doublet_index = torch.tensor(1)\n",
    "\n",
    "    while(True):\n",
    "        logits = current_doublet @ W.T # Logits prediction\n",
    "        counts = logits.exp() # Now we only have positive counts\n",
    "        probs = counts / counts.sum(0, keepdim=True)\n",
    "        # probs = torch.ones(27) / 27 This line is for anyone curious about the difference between a Trigram and random generation\n",
    "\n",
    "        # Sampling next character index:\n",
    "        next_single_char_index = torch.multinomial(probs, num_samples=1)\n",
    "        # If it is * we have reached the end of the name\n",
    "        if next_single_char_index == 0:\n",
    "            break\n",
    "\n",
    "        # Plucking out next character from index:\n",
    "        next_char = itos[next_single_char_index.item()]\n",
    "        # Appending to current name:\n",
    "        current_word.append(next_char)\n",
    "        # Plucking out next doublet input:\n",
    "        next_doublet = current_word[-2:]\n",
    "        next_doublet = ''.join(next_doublet)\n",
    "        next_doublet_index = torch.tensor(doublettoi[next_doublet])\n",
    "        current_doublet = F.one_hot(next_doublet_index, num_classes=702).float()\n",
    "\n",
    "    # Printing the current name:\n",
    "    current_word = ''.join(current_word)\n",
    "    print(current_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's that!\n",
    "Btw I'm really interested in business and AI.\n",
    "For any business ideas please reach out to filiplarssonnnn@gmail.com\n",
    "\n",
    "Bye!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project_env",
   "language": "python",
   "name": "ml_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
